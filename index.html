<!doctype html><html lang=en-ca><head><meta charset=utf-8><title>Random Matrix Theory and Machine Learning Tutorial</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="ICML 2021 tutorial on Random Matrix Theory and Machine Learning"><meta name=author content="Fabian Pedregosa, Courtney Paquette, Tom Trogdon, Jeffrey Pennington"><meta name=generator content="Hugo 0.75.1"><link rel=stylesheet href=https://random-matrix-learning.github.io/plugins/bootstrap/bootstrap.min.css><link rel=stylesheet href=https://random-matrix-learning.github.io/plugins/slick/slick.css><link rel=stylesheet href=https://random-matrix-learning.github.io/plugins/themify-icons/themify-icons.css><link rel=stylesheet href=https://random-matrix-learning.github.io/plugins/magnific-popup/magnific-popup.css><link rel=stylesheet href=https://random-matrix-learning.github.io/scss/style.min.css media=screen><link rel="shortcut icon" href=https://random-matrix-learning.github.io/images/favicon.png type=image/x-icon><link rel=icon href=https://random-matrix-learning.github.io/images/favicon.png type=image/x-icon></head><body><div class=preloader></div><div class="main-navigation fixed-top site-header" id=mainmenu-area><nav class="navbar navbar-expand-lg"><div class="container align-items-center"><a class=navbar-brand href=https://random-matrix-learning.github.io><h2 class="mb-0 text-color">Random Matrix Theory and Machine Learning Tutorial</h2></a><button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarmain aria-controls=navbarmain aria-expanded=false aria-label="Toggle navigation">
<span class=ti-menu-alt></span></button><div class="collapse navbar-collapse text-center text-lg-left" id=navbarmain><ul class="navbar-nav ml-auto"><li class=nav-item><a class="nav-link smoth-scroll" href=#chapters>About</a></li><li class=nav-item><a class="nav-link smoth-scroll" href=#reviews>Instructors</a></li><li class=nav-item><a class="nav-link smoth-scroll" href=#contact>Contact</a></li></div></div></nav></div><section class="banner-main py-7" id=banner><div class=container><div class="row justify-content-between"><div class="col-lg-7 col-md-8"><div class=main-banner><span class="text-color font-weight-bold">ICML 2021 tutorial on</span><h1 class="mb-3 mt-2">Random Matrix Theory and Machine Learning</h1><div class=mb-4><h4><span class=text-color></span></h4></div><p class=mb-4>In recent years, random matrix theory (RMT) has come to the forefront of learning theory as a tool to understand some of its most important challenges. From generalization of deep learning models to a precise analysis of optimization algorithms, RMT provides analytically tractable models.</p><a href=#chapters class="btn btn-main mt-2" role=button>Know more <i class="ti-angle-right ml-3"></i></a></div></div></div></div></section><section class="section chapter" id=chapters><div class=container><div class=row><div><div class=section-heading><h2 class=text-lg>Random Matrix Theory in Machine Learning tutorial.</h2><p>We will present four talks around two cardinal aspects: (1) introducing tools common in RMT that can be applied to machine learning, and (2) Recent applications of RMT in optimization, generalization, and statistical learning theory.</p></div></div></div><div class=row><div class=col-sm-6><div class=chapter-item><h4>Part 1: Introduction and Classical Random Matrix Theory Ensembles</h4><p>In this section we&rsquo;ll discuss applications of Random Matrix Theory to Machine Learning and introduce two of the classical Random Matrix Theory ensembles: the Gaussian Orthogonal Ensemble and Wishart matrices. Through numerical experiments, we&rsquo;ll motivate some of the most important distributions such as the semi-circle and Marchenko-Pastur, as well as key concepts such as universality.</p></div></div><div class=col-sm-6><div class=chapter-item><h4>Part 2: Introduction to random matrix theory, the Stieltjes and R transform</h4><p>In this section we&rsquo;ll will introduce some of the core proof techniques in random matrix theory: the Stieltjes and R transform. Through experiments, we will illustrate how to construct increasingly complex random matrix theory models of real datasets. We&rsquo;ll also discuss some recent topics in random matrix theory such as freeness.</p></div></div><div class=col-sm-6><div class=chapter-item><h4>Part 3: Analysis of numerical algorithms</h4><p>Tom Trogdon will discuss applications of random matrix theory into the analysis of numerical algorithms. We&rsquo;ll see how random matrix theory allows to avoid the pitfals of a worst-case analysis and why the time it takes for an algorithm to return a solution seems to be only mildly dependent on the data distribution over the inputs, a phenomenon known as halting time universality.</p></div></div><div class=col-sm-6><div class=chapter-item><h4>Part 4: The Mystery of Generalization: why Does Deep Learning Work?</h4><p>Jeffrey Pennington will walk us through models of generalization in Deep Neural Networks. With these models we can study some of the most puzzling behaviors of deep neural networks, such as double (and triple) descent.</p></div></div></div></div></section><section class="section chapter" id=presentation1><div class=container><div class=row><div class=section-heading style=margin-bottom:40px><h2 class=text-lg>Part 1. Introduction and Classical Random Matrix Theory Ensembles</h2><p>This section introduces two of the classical Random Matrix Theory ensembles, the Gaussian Orthogonal Ensemble and Wishart matrices. Through numerical experiments, we&rsquo;ll motivate some of the most important distributions in random matrix theory such as the semi-circle and Marchenko-Pastur, as well as key concepts such as universality.</p></div><div style="margin:0 auto"><iframe src=//www.slideshare.net/slideshow/embed_code/key/9DG5ahkpwpZxtH width=595 height=373 frameborder=0 marginwidth=0 marginheight=0 scrolling=no style="display:block;border:1px solid #ccc;border-width:1px;margin-bottom:5px;max-width:100%" allowfullscreen></iframe></div></div><br><p>Extra material:</p><ul><li><a href="https://colab.research.google.com/drive/1Lver5l_9kaWviOW6C2SbCS4y2PTmZOel?usp=sharing">Python experiments (Colab)</a></li><li>Slides PDF (TODO)</li><li>Slides source code (TODO)</li></ul></div></section><section class="section chapter" id=presentation2><div class=container><div class=row><div class=section-heading style=margin-bottom:40px><h2 class=text-lg>Part 2. Introduction to random matrix theory, the Stieltjes and R transform</h2><p>Stieltjes and R transforms</p></div><div style="margin:0 auto"><iframe src=//www.slideshare.net/slideshow/embed_code/key/lqrcDIYulYskwT width=595 height=485 frameborder=0 marginwidth=0 marginheight=0 scrolling=no style="border:1px solid #ccc;border-width:1px;margin-bottom:5px;max-width:100%" allowfullscreen></iframe></div></div></div></section><section class="section chapter" id=presentation3><div class=container><div class=row><div class=section-heading style=margin-bottom:40px><h2 class=text-lg>Part 3. Analysis of numerical algorithms</h2><p>Analysis of Numerical Algorithms</p></div><div style="margin:0 auto"><iframe src=//www.slideshare.net/slideshow/embed_code/key/qK9HNDftMYdpMK width=595 height=485 frameborder=0 marginwidth=0 marginheight=0 scrolling=no style="border:1px solid #ccc;border-width:1px;margin-bottom:5px;max-width:100%" allowfullscreen></iframe></div></div></div></section><section class="section chapter" id=presentation4><div class=container><div class=row><div class=section-heading style=margin-bottom:40px><h2 class=text-lg>Part 4</h2><p>Models for Generalization</p></div><div style="margin:0 auto"><iframe src=//www.slideshare.net/slideshow/embed_code/key/M7R22CVxdfo9qM width=595 height=485 frameborder=0 marginwidth=0 marginheight=0 scrolling=no style="border:1px solid #ccc;border-width:1px;margin-bottom:5px;max-width:100%" allowfullscreen></iframe></div></div></section><section class="section-bottom testimonial" id=reviews><div class=container><div class="row justify-content-center"><div class="col-lg-6 col-md-12"><div class="section-heading text-center"><h2 class="mb-3 text-lg">Instructors</h2><p>The field experts that will guide you through random matrix theory in machine learning are</p></div></div></div><div class="row align-items-center"><div class="col-lg-12 col-sm-12 col-md-12 testimonial-wrap"><div class=test-item><div class=testimonial-item-content><div class="test-author-thumb mb-4"><img src=/images/fff.jpeg alt="Testimonial author" class=img-fluid><div class="test-author-info mt-4"><h4 class="mb-0 mt-2">Fabian Pedregosa</h4><p>Research scientist at Google Research Brain</p></div></div><p class=mb-0>Fabian Pedregosa&rsquo;s research has focused in the last years in developing an average-case analysis of optimization algorithms using techniques from random matrix theory. Previously, he has done research in projection-free and asynchronous optimizationbeen. He is also one of the founding members of the scikit-learn machine learning library. <a href=http://fa.bianp.net>Webpage</a>.</p></div></div><div class=test-item><div class=testimonial-item-content><div class="test-author-thumb mb-4"><img src=/images/ppp.jpeg alt="Testimonial author" class=img-fluid><div class="test-author-info mt-4"><h4 class="mb-0 mt-2">Courtney Paquette</h4><p>Assistant professor at McGill University</p></div></div><p class=mb-0>Courtney Paquette’s research broadly focuses on designing and analyzing algorithms for large-scale optimization problems, motivated by applications in data science. Her recent work has focused on average-case complexity that combined aspects of optimization and random matrix theory. She is a CIFAR Canada AI chair with the Quebec AI institute (MILA).</p></div></div><div class=test-item><div class=testimonial-item-content><div class="test-author-thumb mb-4"><img src=/images/ttt.jpeg alt="Testimonial author" class=img-fluid><div class="test-author-info mt-4"><h4 class="mb-0 mt-2">Thomas Trogdon</h4><p>Associate professor at University of Washington</p></div></div><p class=mb-0>Tom Trogdon’s work focuses on connections between classical algorithms from numerical linear algebra and random matrix theory. He is a core member of a group of researchers that have, first, discovered that the runtimes of algorithms exhibit universal distributions, and second, began rigorously establishing theorems.</p></div></div><div class=test-item><div class=testimonial-item-content><div class="test-author-thumb mb-4"><img src=/images/jjj.jpeg alt="Testimonial author" class=img-fluid><div class="test-author-info mt-4"><h4 class="mb-0 mt-2">Jeffrey Pennington</h4><p>Research scientist at Google Research Brain</p></div></div><p class=mb-0>Jeffrey Pennington&rsquo;s current research interests center on the theory of deep learning, and include topics such as random matrix theory and generalization in high dimensions, the role of overparameterization, wide neural networks and their corresponding kernels, the dynamics of learning, and the geometry of high-dimensional loss surfaces.</p></div></div></div></div></div></section><section class="section contact" id=contact><div class=container><div class="row justify-content-center"><div class="col-md-12 col-lg-8"><div class="section-heading text-center"><h2 class="mb-2 text-lg">Contact Us</h2><p>Whether you have questions or you would just like to say hello, feel free to reach out.</p></div></div></div><div class=row><div class="col-lg-4 col-sm-6"><div class="contact-info-block text-center mb-4"><i class=ti-world></i><p class=mb-0>ICML 2021 tutorial website</p><h5><a href=https://icml.cc/virtual/2021/tutorial/10840>icml.cc</a></h5></div></div><div class="col-lg-4 col-sm-6"><div class="contact-info-block text-center mb-4"><i class=ti-help></i><p class=mb-0>Need help?</p><h5>Read this <a href=https://icml.cc/FAQ>FAQ</a> or contact the ICML <a href=https://icml.cc/Help/Contact>help desk</a></h5></div></div></div></div></section><footer class=footer><div class=container><div class="row justify-content-between align-items-center"><div class="col-lg-6 col-sm-12"><div class=footer-widget><a class="mb-4 d-inline-block" href=https://random-matrix-learning.github.io><h2 class="mb-0 text-dark">Random Matrix Theory and Machine Learning Tutorial</h2></a><p class=mb-4></p></div></div></div></div><div class=container><div class="row footer-btm mt-5 pt-4 border-top"><div class=col-lg-6><p class=footer-copy>© RMT+ML</p></div></div></div></footer><script src=https://random-matrix-learning.github.io/plugins/jQuery/jquery.min.js></script><script src=https://random-matrix-learning.github.io/plugins/bootstrap/bootstrap.min.js></script><script src=https://random-matrix-learning.github.io/plugins/slick/slick.min.js></script><script src=https://random-matrix-learning.github.io/plugins/magnific-popup/magnific-popup.min.js></script><script src=https://random-matrix-learning.github.io/js/script.min.js></script></body></html>