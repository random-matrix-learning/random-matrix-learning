
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title></title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="refs_compiled.tex"> 
<link rel="stylesheet" type="text/css" href="refs_compiled.css"> 
</head><body 
>
   <h3 class="likesectionHead"><a 
 id="x1-1000"></a>References</h3>
<!--l. 1--><p class="noindent" >
    <div class="thebibliography">
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xadlam2019random"></a>[1] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ben Adlam, Jake Levinson, and Jeffrey Pennington. A random matrix
    perspective on mixtures of nonlinearities for deep learning. <span 
class="cmti-10">arXiv preprint</span>
    <span 
class="cmti-10">arXiv:1912.00827</span>, 2019.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xadlam2020neural"></a>[2] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ben Adlam and Jeffrey Pennington. The neural tangent kernel in high
    dimensions: Triple descent and a multi-scale theory of generalization.  In
    <span 
class="cmti-10">International Conference on Machine Learning</span>. PMLR, 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xadvani2020high"></a>[3] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Madhu&#x00A0;S       Advani,       Andrew&#x00A0;M       Saxe,       and       Haim
    Sompolinsky. High-dimensional dynamics of generalization error in neural
    networks. <span 
class="cmti-10">Neural Networks</span>, 132:428&#8211;446, 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xamit1985spin"></a>[4] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Daniel&#x00A0;J Amit, Hanoch Gutfreund, and Haim Sompolinsky. Spin-glass
    models of neural networks. <span 
class="cmti-10">Physical Review A</span>, 1985.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xanderson2010introduction"></a>[5] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Greg&#x00A0;W Anderson, Alice Guionnet, and Ofer Zeitouni. <span 
class="cmti-10">An introduction</span>
    <span 
class="cmti-10">to random matrices</span>. Cambridge university press, 2010.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbai2010spectral"></a>[6] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Z.&#x00A0;Bai                              and                              J.&#x00A0;Silverstein.
    <span 
class="cmti-10">https://link.springer.com/book/10.1007%2F978-1-4419-0661-8Spectral</span>
    <span 
class="cmti-10">analysis of large dimensional random matrices</span>, volume&#x00A0;20. Springer, 2010.

    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbai2005CLT"></a>[7] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Z.&#x00A0;D.  Bai  and  Jack&#x00A0;W.  Silverstein.      CLT  for  linear  spectral
    statistics of large-dimensional sample covariance matrices.  <span 
class="cmti-10">Ann. Probab.</span>,
    32(1A):553&#8211;605, 2004.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="XBaik2005"></a>[8] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>J&#x00A0;Baik, G&#x00A0;Ben Arous, and S&#x00A0;Péché.  Phase transition of the largest
    eigenvalue for nonnull complex sample covariance matrices. <span 
class="cmti-10">The Annals of</span>
    <span 
class="cmti-10">Probability</span>, 33(5), sep 2005.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbaik2006eigenvalues"></a>[9] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jinho  Baik  and  Jack&#x00A0;W.  Silverstein.    Eigenvalues  of  large  sample
    covariance matrices of spiked population models.  <span 
class="cmti-10">J. Multivariate Anal.</span>,
    97(6):1382&#8211;1408, 2006.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbaity2018comparing"></a>[10] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Marco  Baity-Jesi,  Levent  Sagun,  Mario  Geiger,  Stefano  Spigler,
    Gérard&#x00A0;Ben Arous, Chiara Cammarota, Yann LeCun, Matthieu Wyart,
    and Giulio Biroli. Comparing dynamics: Deep neural networks versus glassy
    systems. In <span 
class="cmti-10">International Conference on Machine Learning</span>. PMLR, 2018.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbartlett2020benign"></a>[11] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Peter&#x00A0;L Bartlett, Philip&#x00A0;M Long, Gábor Lugosi, and Alexander Tsigler.
    Benign overfitting in linear regression. <span 
class="cmti-10">Proceedings of the National Academy</span>
    <span 
class="cmti-10">of Sciences</span>, 117(48):30063&#8211;30070, 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbaskerville2021applicability"></a>[12] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Nicholas&#x00A0;P  Baskerville,  Diego  Granziol,  and  Jonathan&#x00A0;P  Keating.
    Applicability of random matrix theory in deep learning.  <span 
class="cmti-10">arXiv preprint</span>
    <span 
class="cmti-10">arXiv:2102.06740</span>, 2021.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbelkin2019reconciling"></a>[13] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mikhail    Belkin,    Daniel    Hsu,    Siyuan    Ma,    and    Soumik
    Mandal.  Reconciling modern machine-learning practice and the classical
    bias&#8211;variance trade-off.  <span 
class="cmti-10">Proceedings of the National Academy of Sciences</span>,
    116(32):15849&#8211;15854, 2019.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbelkin2020two"></a>[14] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mikhail Belkin, Daniel Hsu, and Ji&#x00A0;Xu. Two models of double descent
    for  weak  features.    <span 
class="cmti-10">SIAM  Journal  on  Mathematics  of  Data  Science</span>,
    2(4):1167&#8211;1180, 2020.

    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbelkin2018understand"></a>[15] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep
    learning we need to understand kernel learning. In <span 
class="cmti-10">International Conference</span>
    <span 
class="cmti-10">on Machine Learning</span>, pages 541&#8211;549. PMLR, 2018.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XBloemendal2013"></a>[16] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>A&#x00A0;Bloemendal  and  B&#x00A0;Virág.   Limits  of  spiked  random  matrices  I.
    <span 
class="cmti-10">Probability Theory and Related Fields</span>, 156(3-4):795&#8211;825, aug 2013.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XBorgwardt1987"></a>[17] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>K&#x00A0;H  Borgwardt.     <span 
class="cmti-10">The  simplex  method:  A  probabilistic  analysis</span>.
    Springer&#8211;Verlag, Berlin, Heidelberg, 1987.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbouchaud2009financial"></a>[18] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jean-Philippe Bouchaud and Marc Potters.  Financial applications of
    random matrix theory: a short review.  <span 
class="cmti-10">arXiv preprint arXiv:0910.1205</span>,
    2009.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XBourgade2014"></a>[19] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>P&#x00A0;Bourgade,  L&#x00A0;Erd&#x0151;s,  and  H-T  Yau.    Edge  Universality  of  Beta
    Ensembles. <span 
class="cmti-10">Communications in Mathematical Physics</span>, 332(1):261&#8211;353, nov
    2014.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbun2016cleaning"></a>[20] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Joël Bun, Jean-Philippe Bouchaud, and Marc Potters.  Cleaning large
    correlation  matrices:  tools  from  random  matrix  theory.    <span 
class="cmti-10">Phys.  Rep.</span>,
    666:1&#8211;109, 2017.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xcanziani2016analysis"></a>[21] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis
    of deep neural network models for practical applications.  <span 
class="cmti-10">arXiv preprint</span>
    <span 
class="cmti-10">arXiv:1605.07678</span>, 2016.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xchoromanska2015loss"></a>[22] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Anna  Choromanska,  Mikael  Henaff,  Michael  Mathieu,  Gérard&#x00A0;Ben
    Arous, and Yann LeCun.   The loss surfaces of multilayer networks.   In
    <span 
class="cmti-10">Artificial intelligence and statistics</span>. PMLR, 2015.
    </p>

    <p class="bibitem" ><span class="biblabel">
<a 
 id="XDadush2020"></a>[23] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>
    D&#x00A0;Dadush and S&#x00A0;Huiberts. A Friendly Smoothed Analysis of the Simplex
    Method. <span 
class="cmti-10">SIAM Journal on Computing</span>, 49(5):STOC18&#8211;449&#8211;STOC18&#8211;499,
    jan 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XDantzig1990"></a>[24] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>G&#x00A0;B Dantzig. Origins of the simplex method. In <span 
class="cmti-10">A history of scientific</span>
    <span 
class="cmti-10">computing</span>, pages 141&#8211;151. ACM, New York, NY, USA, jun 1990.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdauphin2014identifying"></a>[25] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho,
    Surya Ganguli, and Yoshua Bengio.  Identifying and attacking the saddle
    point problem in high-dimensional non-convex optimization. <span 
class="cmti-10">arXiv preprint</span>
    <span 
class="cmti-10">arXiv:1406.2572</span>, 2014.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XDeiftOrthogonalPolynomials"></a>[26] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>P&#x00A0;Deift.       <span 
class="cmti-10">Orthogonal   Polynomials   and   Random   Matrices:   a</span>
    <span 
class="cmti-10">Riemann-Hilbert Approach</span>. Amer. Math. Soc., Providence, RI, 2000.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XDeift2017"></a>[27] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>P&#x00A0;Deift and T&#x00A0;Trogdon.   Universality for Eigenvalue Algorithms on
    Sample Covariance Matrices. <span 
class="cmti-10">SIAM Journal on Numerical Analysis</span>, 2017.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdeift2014universality"></a>[28] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Percy&#x00A0;A Deift, Govind Menon, Sheehan Olver, and Thomas Trogdon.
    Universality in numerical computations with random data. <span 
class="cmti-10">Proceedings of</span>
    <span 
class="cmti-10">the National Academy of Sciences</span>, 2014.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XDeshpande"></a>[29] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>A&#x00A0;Deshpande and D&#x00A0;A Spielman. Improved Smoothed Analysis of the
    Shadow Vertex Simplex Method.   In <span 
class="cmti-10">46th Annual IEEE Symposium on</span>
    <span 
class="cmti-10">Foundations of Computer Science (FOCS&#8217;05)</span>, pages 349&#8211;356. IEEE, 2005.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XDing2021"></a>[30] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>X&#x00A0;Ding and T&#x00A0;Trogdon. The conjugate gradient algorithm on a general
    class of spiked covariance matrices.  <span 
class="cmti-10">arXiv preprint arXiv:2106.13902</span>, jun
    2021.
    </p>

    <p class="bibitem" ><span class="biblabel">
<a 
 id="XDing2019"></a>[31] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>X&#x00A0;Ding  and  F&#x00A0;Yang.    Spiked  separable  covariance  matrices  and
    principal components. <span 
class="cmti-10">arXiv preprint arXiv:1905.13060</span>, may 2019.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdomingo2020average"></a>[32] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Carles   Domingo-Enrich,   Fabian   Pedregosa,   and   Damien   Scieur.
    Average-case acceleration for bilinear games and normal matrices.  <span 
class="cmti-10">arXiv</span>
    <span 
class="cmti-10">preprint arXiv:2010.02076</span>, 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdotsenko1995introduction"></a>[33] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Viktor Dotsenko.   <span 
class="cmti-10">An introduction to the theory of spin glasses and</span>
    <span 
class="cmti-10">neural networks</span>. World Scientific, 1995.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdrineas2018lectures"></a>[34] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Petros  Drineas  and  Michael&#x00A0;W  Mahoney.   Lectures  on  randomized
    numerical linear algebra. <span 
class="cmti-10">The Mathematics of Data</span>, 2018.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdrineas2005nystrom"></a>[35] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Petros  Drineas,  Michael&#x00A0;W  Mahoney,  and  Nello  Cristianini.    On
    the  nyström  method  for  approximating  a  gram  matrix  for  improved
    kernel-based learning. <span 
class="cmti-10">journal of machine learning research</span>, 2005.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XEr01"></a>[36] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>P.&#x00A0;Erd&#x0151;s.  A selection of problems and results in combinatorics.  In
    <span 
class="cmti-10">Recent trends in combinatorics (Matrahaza, 1995)</span>, pages 1&#8211;6. Cambridge
    Univ. Press, Cambridge, 1995.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xerdos1960evolution"></a>[37] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Paul Erdos and Alfréd Rényi.   On the evolution of random graphs.
    <span 
class="cmti-10">Publ. Math. Inst. Hung. Acad. Sci</span>, 1960.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XErdos2013a"></a>[38] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>L&#x00A0;Erd&#x0151;s,  A&#x00A0;Knowles,  H-T  Yau,  and  J&#x00A0;Yin.   Spectral  statistics  of
    Erd&#x0151;s&#8211;Rényi graphs I: Local semicircle law.  <span 
class="cmti-10">The Annals of Probability</span>,
    41(3B), may 2013.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xgardner1988optimal"></a>[39] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Elizabeth Gardner and Bernard Derrida. Optimal storage properties of
    neural network models.  <span 
class="cmti-10">Journal of Physics A: Mathematical and general</span>,
    1988.

    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xghorbani2019investigation"></a>[40] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation
    into neural net optimization via hessian eigenvalue density. In <span 
class="cmti-10">International</span>
    <span 
class="cmti-10">Conference on Machine Learning</span>. PMLR, 2019.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xgoldstine1951numerical"></a>[41] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Herman&#x00A0;H Goldstine and John Von&#x00A0;Neumann.  Numerical inverting
    of matrices of high order. ii.  <span 
class="cmti-10">Proceedings of the American Mathematical</span>
    <span 
class="cmti-10">Society</span>, 1951.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xgower2015randomized"></a>[42] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Robert&#x00A0;M Gower and Peter Richtárik. Randomized iterative methods
    for linear systems.  <span 
class="cmti-10">SIAM Journal on Matrix Analysis and Applications</span>,
    2015.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XConcreteMath"></a>[43] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>R.L. Graham, D.E. Knuth, and O.&#x00A0;Patashnik. <span 
class="cmti-10">Concrete mathematics</span>.
    Addison-Wesley, Reading, MA, 1989.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xgreenwade93"></a>[44] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>George&#x00A0;D.  Greenwade.    The  Comprehensive  Tex  Archive  Network
    (CTAN). <span 
class="cmti-10">TUGBoat</span>, 14(3):342&#8211;351, 1993.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xhalko2011finding"></a>[45] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Nathan Halko, Per-Gunnar Martinsson, and Joel&#x00A0;A Tropp.  Finding
    structure  with  randomness:  Probabilistic  algorithms  for  constructing
    approximate matrix decompositions. <span 
class="cmti-10">SIAM review</span>, 2011.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xhastie2019surprises"></a>[46] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Trevor  Hastie,  Andrea  Montanari,  Saharon  Rosset,  and  Ryan&#x00A0;J
    Tibshirani.      Surprises   in   high-dimensional   ridgeless   least   squares
    interpolation. <span 
class="cmti-10">arXiv preprint arXiv:1903.08560</span>, 2019.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XHestenes1952"></a>[47] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>M&#x00A0;Hestenes and E&#x00A0;Steifel. Method of Conjugate Gradients for Solving
    Linear Systems. <span 
class="cmti-10">J. Research Nat. Bur. Standards</span>, 20:409&#8211;436, 1952.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xjacot2020kernel"></a>[48] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Arthur  Jacot,  Berfin  &#350;im&#351;ek,  Francesco  Spadaro,  Clément  Hongler,
    and Franck Gabriel. Kernel alignment risk estimator: risk prediction from
    training data. <span 
class="cmti-10">arXiv preprint arXiv:2006.09796</span>, 2020.

    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkaczmarz1937angenaherte"></a>[49] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>S&#x00A0;Kaczmarz. Angenäherte auflösung von systemen linearer gleichungen
    (english  translation  by  jason  stockmann):  Bulletin  international  de
    l&#8217;académie polonaise des sciences et des lettres. 1937.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkeating1993riemann"></a>[50] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>J&#x00A0;Keating.   The  Riemann  zeta-function  and  quantum  chaology.   In
    <span 
class="cmti-10">Quantum chaos</span>. Elsevier, 1993.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XKnowles2017"></a>[51] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>A&#x00A0;Knowles and J&#x00A0;Yin.  Anisotropic local laws for random matrices.
    <span 
class="cmti-10">Probability Theory and Related Fields</span>, 169(1-2):257&#8211;352, oct 2017.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xknuth92"></a>[52] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>D.E. Knuth. Two notes on notation. <span 
class="cmti-10">Amer. Math. Monthly</span>, 99:403&#8211;422,
    1992.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XKostlan1988"></a>[53] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>E&#x00A0;Kostlan. Complexity theory of numerical linear algebra. <span 
class="cmti-10">Journal of</span>
    <span 
class="cmti-10">Computational and Applied Mathematics</span>, 22(2-3):219&#8211;230, jun 1988.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XKuczynski1992"></a>[54] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>J&#x00A0;Kuczy&#324;ski and H&#x00A0;Wo&#378;niakowski. Estimating the Largest Eigenvalue
    by the Power and Lanczos Algorithms with a Random Start. <span 
class="cmti-10">SIAM Journal</span>
    <span 
class="cmti-10">on Matrix Analysis and Applications</span>, 13(4):1094&#8211;1122, oct 1992.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlacotte2020optimal"></a>[55] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jonathan Lacotte and Mert Pilanci.  Optimal randomized first-order
    methods  for  least-squares  problems.    In  <span 
class="cmti-10">International  Conference  on</span>
    <span 
class="cmti-10">Machine Learning</span>. PMLR, 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xliang2020just"></a>[56] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tengyuan  Liang  and  Alexander  Rakhlin.   Just  interpolate:  Kernel
    &#8220;ridgeless&#8221;   regression   can   generalize.       <span 
class="cmti-10">The   Annals   of   Statistics</span>,
    48(3):1329&#8211;1347, 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XLiao2021"></a>[57] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Z&#x00A0;Liao and M&#x00A0;W Mahoney.  Hessian Eigenspectra of More Realistic
    Nonlinear Models. mar 2021.

    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xliao2019random"></a>[58] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Zhenyu  Liao.    <span 
class="cmti-10">A  random  matrix  framework  for  large  dimensional</span>
    <span 
class="cmti-10">machine learning and neural networks</span>. PhD thesis, Université Paris-Saclay,
    2019.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xliao2020random"></a>[59] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Zhenyu Liao, Romain Couillet, and Michael&#x00A0;W Mahoney.  A random
    matrix analysis of random fourier features: beyond the gaussian kernel,
    a precise phase transition, and the corresponding double descent.  <span 
class="cmti-10">arXiv</span>
    <span 
class="cmti-10">preprint arXiv:2006.05013</span>, 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmarvcenko1967distribution"></a>[60] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Vladimir&#x00A0;A Mar&#x010D;enko and Leonid&#x00A0;Andreevich Pastur.   Distribution
    of  eigenvalues  for  some  sets  of  random  matrices.   <span 
class="cmti-10">Mathematics  of  the</span>
    <span 
class="cmti-10">USSR-Sbornik</span>, 1967.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmehta2004random"></a>[61] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Madan&#x00A0;Lal Mehta. <span 
class="cmti-10">Random matrices</span>. Elsevier, 2004.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmei2019generalization"></a>[62] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Song Mei and Andrea Montanari.  The generalization error of random
    features regression: Precise asymptotics and double descent curve.  <span 
class="cmti-10">arXiv</span>
    <span 
class="cmti-10">preprint arXiv:1908.05355</span>, 2019.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmingo2017free"></a>[63] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>James&#x00A0;A Mingo and Roland Speicher.   <span 
class="cmti-10">Free probability and random</span>
    <span 
class="cmti-10">matrices</span>, volume&#x00A0;35. Springer, 2017.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmitra2019understanding"></a>[64] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Partha&#x00A0;P  Mitra.   Understanding  overfitting  peaks  in  generalization
    error: Analytical risk curves for <span 
class="cmmi-10">l</span>_2 and <span 
class="cmmi-10">l</span>_1 penalized interpolation. <span 
class="cmti-10">arXiv</span>
    <span 
class="cmti-10">preprint arXiv:1906.03667</span>, 2019.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmontgomery1973pair"></a>[65] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Hugh&#x00A0;L Montgomery. The pair correlation of zeros of the zeta function.
    In <span 
class="cmti-10">Proc. Symp. Pure Math</span>, 1973.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xnakkiran2019deep"></a>[66] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz
    Barak, and Ilya Sutskever. Deep double descent: Where bigger models and
    more data hurt. <span 
class="cmti-10">arXiv preprint arXiv:1912.02292</span>, 2019.

    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xodlyzko1987distribution"></a>[67] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Andrew&#x00A0;M Odlyzko.  On the distribution of spacings between zeros of
    the zeta function. <span 
class="cmti-10">Mathematics of Computation</span>, 1987.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XJMLR:v21:20-933"></a>[68] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Vardan  Papyan.   Traces  of  class/cross-class  structure  pervade  deep
    learning spectra. <span 
class="cmti-10">Journal of Machine Learning Research</span>, 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XPaquette2021"></a>[69] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>C&#x00A0;Paquette,  K&#x00A0;Lee,  F&#x00A0;Pedregosa,  and  E&#x00A0;Paquette.    SGD  in  the
    Large: Average-case Analysis, Asymptotics, and Stepsize Criticality. <span 
class="cmti-10">arXiv</span>
    <span 
class="cmti-10">preprint 2102.04396</span>, feb 2021.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XPaquette2020a"></a>[70] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>C&#x00A0;Paquette, B&#x00A0;van Merriënboer, and F&#x00A0;Pedregosa.  Halting Time is
    Predictable for Large Models: A Universality Property and Average-case
    Analysis. <span 
class="cmti-10">arXiv preprint arXiv:2006.04299</span>, jun 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xpaquette2020universality"></a>[71] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Elliot Paquette and Thomas Trogdon.  Universality for the conjugate
    gradient  and  minres  algorithms  on  sample  covariance  matrices.   <span 
class="cmti-10">arXiv</span>
    <span 
class="cmti-10">preprint arXiv:2007.00640</span>, 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xpeche2019note"></a>[72] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>S&#x00A0;Péché et&#x00A0;al. A note on the pennington-worah distribution. <span 
class="cmti-10">Electronic</span>
    <span 
class="cmti-10">Communications in Probability</span>, 2019.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xpedregosa2020acceleration"></a>[73] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Fabian Pedregosa and Damien Scieur.  Acceleration through spectral
    density  estimation.   In  <span 
class="cmti-10">International Conference on Machine Learning</span>.
    PMLR, 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xpennington2017geometry"></a>[74] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jeffrey Pennington and Yasaman Bahri.  Geometry of neural network
    loss  surfaces  via  random  matrix  theory.    In  <span 
class="cmti-10">Proceedings  of  the  34th</span>
    <span 
class="cmti-10">International Conference on Machine Learning</span>, volume&#x00A0;70 of <span 
class="cmti-10">Proceedings</span>
    <span 
class="cmti-10">of Machine Learning Research</span>, pages 2798&#8211;2806. PMLR, 2017.
    </p>

    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xpennington2017nonlinear"></a>[75] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory
    for deep learning. 2017.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XDiagonalRMT"></a>[76] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>C&#x00A0;W Pfrang, P&#x00A0;Deift, and G&#x00A0;Menon. How long does it take to compute
    the eigenvalues of a random symmetric matrix?  <span 
class="cmti-10">Random matrix theory,</span>
    <span 
class="cmti-10">interacting particle systems, and integrable systems, MSRI Publications</span>,
    65:411&#8211;442, 2014.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrakhlin2019consistency"></a>[77] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alexander Rakhlin and Xiyu Zhai.  Consistency of interpolation with
    laplace  kernels  is  a  high-dimensional  phenomenon.    In  <span 
class="cmti-10">Conference  on</span>
    <span 
class="cmti-10">Learning Theory</span>, pages 2595&#8211;2623. PMLR, 2019.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XPhysRev.120.1698"></a>[78] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Norbert Rosenzweig and Charles&#x00A0;E. Porter. &#8221;repulsion of energy levels&#8221;
    in complex atomic spectra. <span 
class="cmti-10">Phys. Rev.</span>, 1960.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrudi2015less"></a>[79] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alessandro  Rudi,  Raffaello  Camoriano,  and  Lorenzo  Rosasco.   Less
    is more: Nyström computational regularization.   In <span 
class="cmti-10">Advances in Neural</span>
    <span 
class="cmti-10">Information Processing Systems</span>, 2015.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsagun2014explorations"></a>[80] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Levent   Sagun,   V&#x00A0;Ugur   Guney,   Gerard&#x00A0;Ben   Arous,   and   Yann
    LeCun.   Explorations  on  high  dimensional  landscapes.   <span 
class="cmti-10">arXiv  preprint</span>
    <span 
class="cmti-10">arXiv:1412.6615</span>, 2014.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsagun2015universal"></a>[81] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Levent Sagun, Thomas Trogdon, and Yann LeCun.  Universal halting
    times  in  optimization  and  machine  learning.     <span 
class="cmti-10">Quarterly  of  Applied</span>
    <span 
class="cmti-10">Mathematics</span>, 2017.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XSimpson"></a>[82] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>H.&#x00A0;Simpson.   Proof  of  the  Riemann  Hypothesis.   preprint  (2003),
    available at <span 
class="cmtt-10">http://www.math.drofnats.edu/riemann.ps</span>, 2003.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XSmale1983"></a>[83] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>S&#x00A0;Smale.  On the average number of steps of the simplex method of
    linear programming. <span 
class="cmti-10">Mathematical Programming</span>, 27(3):241&#8211;262, oct 1983.

    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XSpielman2004"></a>[84] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>D&#x00A0;A Spielman and S-H Teng. Smoothed analysis of algorithms. <span 
class="cmti-10">Journal</span>
    <span 
class="cmti-10">of the ACM</span>, 51(3):385&#8211;463, may 2004.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xspielman2011spectral"></a>[85] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Daniel&#x00A0;A Spielman and Shang-Hua Teng.   Spectral sparsification of
    graphs. <span 
class="cmti-10">SIAM Journal on Computing</span>, 2011.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XStrohmer2009"></a>[86] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>T&#x00A0;Strohmer and R&#x00A0;Vershynin.  A Randomized Kaczmarz Algorithm
    with  Exponential  Convergence.      <span 
class="cmti-10">Journal  of  Fourier  Analysis  and</span>
    <span 
class="cmti-10">Applications</span>, 15(2):262&#8211;278, apr 2009.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xtulino2004random"></a>[87] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Antonia&#x00A0;M Tulino, Sergio Verdú, and Sergio Verdu.  <span 
class="cmti-10">Random matrix</span>
    <span 
class="cmti-10">theory and wireless communications</span>. Now Publishers Inc, 2004.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XVershynin2009a"></a>[88] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>R&#x00A0;Vershynin. Beyond Hirsch Conjecture: Walks on Random Polytopes
    and  Smoothed  Complexity  of  the  Simplex  Method.   <span 
class="cmti-10">SIAM Journal on</span>
    <span 
class="cmti-10">Computing</span>, 39, 2009.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xwigner1955characteristic"></a>[89] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Eugene  Wigner.    Characteristic  vectors  of  bordered  matrices  with
    infinite dimensions. <span 
class="cmti-10">Annals of Mathematics</span>, 1955.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xwilliams2001using"></a>[90] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Christopher Williams and Matthias Seeger. Using the nyström method
    to speed up kernel machines. In <span 
class="cmti-10">Proceedings of the 14th annual conference</span>
    <span 
class="cmti-10">on neural information processing systems</span>, 2001.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xwishart1928generalised"></a>[91] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>John Wishart. The generalised product moment distribution in samples
    from a normal multivariate population. <span 
class="cmti-10">Biometrika</span>, 1928.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xyao2020pyhessian"></a>[92] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael&#x00A0;W Mahoney.
    Pyhessian: Neural networks through the lens of the hessian. In <span 
class="cmti-10">2020 IEEE</span>
    <span 
class="cmti-10">International Conference on Big Data (Big Data)</span>. IEEE, 2020.

    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xzhang2021understanding"></a>[93] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Chiyuan  Zhang,  Samy  Bengio,  Moritz  Hardt,  Benjamin  Recht,  and
    Oriol  Vinyals.   Understanding  deep  learning  (still)  requires  rethinking
    generalization. <span 
class="cmti-10">Communications of the ACM</span>, 64(3):107&#8211;115, 2021.
</p>
    </div>
    
</body></html> 



